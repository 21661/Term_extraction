POST http://127.0.0.1:8000/extract
Content-Type: application/json

{
  "summary": "Multimodal (hyperspectral + panchromatic/optical) remote sensing image semantic segmentation â€” emphasizing cross-modal feature fusion, adaptive attention mechanisms, hybrid loss design, and multi-scale supervision to improve segmentation accuracy and generalization across domains",
  "chunks": [
    {
      "1":"This study proposes a multimodal deep learning framework for semantic segmentation of remote sensing imagery, aiming to address the well-known trade-off between the spatial resolution of panchromatic images and the spectral richness of hyperspectral data. We construct a dual-branch encoder in which one branch focuses on hyperspectral feature extraction and the other on panchromatic spatial enhancement."
    },
    {
      "2": "In terms of architectural details, the encoder adopts a hierarchical pyramid design with progressive down-sampling to capture features at multiple spatial resolutions. Within each pyramid level, a lightweight spectral-band selection module is embedded to dynamically emphasize or suppress specific wavelength ranges using learnable scaling factors, thus maintaining spectral discriminability in noisy or heterogeneous environments. The fusion module incorporates relative positional encoding and normalized attention to reduce alignment errors caused by geometric and viewing angle discrepancies between sensors. To support large-scale inference, a sparse attention approximation strategy is proposed, combining local and interleaved global windows to approximate full attention while significantly reducing time and memory complexity. On the training side, we adopt a progressive learning rate schedule, class-balanced sampling, and a self-supervised reconstruction pretext task to exploit the intrinsic structure of hyperspectral data. "
    }
  ]
}

from typing import List, TypedDict, Dict
from langgraph.graph import StateGraph, END
import spacy
import json
import time
import logging
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
from utils.zhipu_client import client
from utils.db_interface import query_term_translation, save_translation
from utils.web_fetcher import fetch_core_translations
from utils.Get_term import translate_term as translate_term_external, get_translation_candidates_batch,translate_term
from utils.workflow_adapter import _rewrap,_unwrap
from utils.extract_candidates import extract_candidates
from utils.TimeNode import timed_node
from utils.candidate_tool import normalize_candidate, is_noise_candidate, dedupe_keep_longest, _MAX_TERMS_TO_PROCESS, _LLM_RETRIES, _RETRY_BACKOFF, _COMMON_GENERIC_WORDS
import typing

# ===================== 1ï¸âƒ£ å®šä¹‰çŠ¶æ€ç±»å‹ =====================
class TermState(TypedDict):
    text: str
    candidates: List[str]
    terms: List[str]
    topic: str
    translations: Dict[str, List[str]]  # term -> å€™é€‰ç¿»è¯‘
    final_translations: Dict[str, str]


# ===================== 2ï¸âƒ£ åˆå§‹åŒ– =====================
nlp = spacy.load("en_core_web_trf")

# logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)



# ----------------- Helpers (no caching) -----------------
def llm_completion(prompt: str, system: str = "ä½ æ˜¯æœ¯è¯­åˆ†ç±»åŠ©æ‰‹") -> str:
    """ç›´æ¥è°ƒç”¨ LLMï¼ŒåŒæ­¥é˜»å¡ï¼Œä¸åšç¼“å­˜æˆ–è¶…æ—¶ä¿æŠ¤ã€‚"""
    try:
        completion = client.chat.completions.create(
            model="glm-4.5-flash",
            messages=[{"role": "system", "content": system}, {"role": "user", "content": prompt}],
            temperature=0,
        )
        return _safe_extract_completion_content(completion)
    except Exception as e:
        logger.warning("LLM direct call failed: %s", e)
        return "None"
    # no caching or decorator returned; function ends here
# ===================== 5ï¸âƒ£ åˆ¤æ–­æŠ“å–æ–¹å¼ =====================
def decide_post_db_method(term: str) -> str:
    term_lower = term.lower()
    simple_words = {"data", "set", "map", "input", "output", "value", "use", "function"}
    if term_lower in simple_words or len(term) <= 3:
        return "dict"
    if len(term.split()) > 1 or re.search(r"[-_/]", term):
        return "llm"
    return "dict"

def _safe_extract_completion_content(completion) -> str:
    """Robustly extract text/content from various completion response shapes.
    Returns empty string when content is missing or blank.
    """
    if not completion:
        return ""
    try:
        # common OpenAI-like shape
        choices = None
        if isinstance(completion, dict):
            choices = completion.get('choices')
        else:
            choices = getattr(completion, 'choices', None)
        if choices:
            first = choices[0]
            if isinstance(first, dict):
                msg = first.get('message') or first.get('text')
            else:
                msg = getattr(first, 'message', None) or getattr(first, 'text', None)
            if not msg:
                return ""
            if isinstance(msg, dict):
                return (msg.get('content') or msg.get('text') or "").strip()
            else:
                return str(getattr(msg, 'content', None) or msg).strip()
        # fallback for direct attribute
        if isinstance(completion, dict):
            return (completion.get('text') or "").strip()
        return (getattr(completion, 'text', '') or '').strip()
    except Exception:
        return ""


@timed_node()
def translate_node(state: typing.Any) -> TermState:
    """Concurrent translation of detected terms with caching and timeouts.
    æ”¹ä¸ºåªç¿»è¯‘ state['selected_terms']ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼Œå¦åˆ™é€€å›åˆ° state['terms']ã€‚
    æ”¯æŒæ‰¹é‡ LLM ç¿»è¯‘ä»¥å‡å°‘è¯·æ±‚æ¬¡æ•°ã€‚
    """
    original = state
    inner, parent, key = _unwrap(state)
    state = inner if isinstance(inner, dict) else {"terms": []}
    state_dict: dict = typing.cast(dict, state)

    translations: Dict[str, List[str]] = {}
    final_translations: Dict[str, str] = {}

    # ä¼˜å…ˆä½¿ç”¨ selected_termsï¼ˆç”± select_top_terms èŠ‚ç‚¹ç”Ÿæˆï¼‰
    terms = state_dict.get("selected_terms") or state_dict.get("terms") or []
    topic = state_dict.get("topic") or state_dict.get("summary") or ""
    if not terms:
        state_dict["translations"] = translations
        state_dict["final_translations"] = final_translations
        return _rewrap(original, parent, key, state_dict)

    # -----------------------------
    # ğŸ”¹ æ–°å¢ï¼šæ‰¹é‡è°ƒç”¨æœ¬åœ°ç¿»è¯‘æ¥å£ï¼ˆå« LLM æ‰¹é‡ç¿»è¯‘ï¼‰ï¼Œä¼ å…¥ topic
    # -----------------------------
    try:
        from utils.Get_term import get_translation_candidates_batch as _batch
        batch_result = _batch(terms, topic=topic)
    except Exception as e:
        logger.warning("æ‰¹é‡ç¿»è¯‘å¤±è´¥ï¼Œå›é€€åˆ°å•é¡¹æ¨¡å¼: %s", e)
        batch_result = {}

    # -----------------------------
    # ğŸ”¹ å¯¹æ¯ä¸ª term è¿›è¡Œæ¸…ç†ã€é™çº§å›é€€
    # -----------------------------
    for term in terms:
        candidates = batch_result.get(term, [])

        # å¦‚æœæ‰¹é‡ç¿»è¯‘ä¸ºç©ºï¼Œåˆ™å°è¯•æ—§é€»è¾‘ï¼ˆç¼“å­˜æˆ– web æŠ“å–ï¼‰
        if not candidates:
            try:
                from utils.Get_term import translate_term as translate_term_external
                candidates = translate_term_external(term, topic=topic)
            except Exception as e:
                logger.warning("å•é¡¹å›é€€ç¿»è¯‘å¤±è´¥ %s: %s", term, e)
                candidates = []

        # clean + dedupe preserving order
        seen = set()
        cleaned = []
        for c in candidates:
            if not c:
                continue
            c_clean = str(c).strip()
            if not c_clean or c_clean in seen:
                continue
            seen.add(c_clean)
            cleaned.append(c_clean)

        translations[term] = cleaned
        if cleaned:
            final_translations[term] = cleaned[0]
        else:
            try:
                local = query_term_translation(term) or []
                if local:
                    translations[term] = local
                    final_translations[term] = local[0]
            except Exception:
                pass

    state_dict["translations"] = translations
    state_dict["final_translations"] = final_translations
    result = _rewrap(original, parent, key, state_dict)
    return result



# ===================== æ–°å¢èŠ‚ç‚¹ï¼šå…ˆé€‰å‡ºæœ€ç»ˆè¦ç¿»è¯‘çš„ top-N æœ¯è¯­ =====================
@timed_node()
def select_top_terms(state: typing.Any) -> TermState:
    """ä» state['terms'] ä¸­é€‰å‡ºæœ€å¤š _MAX_TERMS_TO_PROCESS ä¸ªé«˜è´¨é‡æœ¯è¯­ï¼Œè¾“å‡ºåˆ° state['selected_terms']ã€‚

    ä¿æŒæœ¯è¯­çš„åŸå§‹å¤§å°å†™ï¼›åªåœ¨å†…éƒ¨æ¯”è¾ƒ/å»é‡æ—¶ä½¿ç”¨ normalize_candidateã€‚
    """
    original = state
    inner, parent, key = _unwrap(state)
    state = inner if isinstance(inner, dict) else {"terms": []}
    state_dict: dict = typing.cast(dict, state)

    # å°è¯•ä»å·²æœ‰çš„ terms è·å–ï¼›è‹¥æ²¡æœ‰ï¼Œåˆ™é€€å›åˆ° candidates
    terms = list(state_dict.get("terms") or [])
    topic = state_dict.get("topic", "")

    # è‹¥ terms ä¸ºç©ºï¼Œä» candidates åŸºäºåŸå§‹å­—ç¬¦ä¸²æ„å»ºï¼Œå™ªå£°è¿‡æ»¤å’Œå»é‡æ—¶ç”¨ normalize_candidate
    if not terms:
        candidates = state_dict.get("candidates") or []
        filtered_original: list[str] = []
        for c in candidates:
            if not isinstance(c, str):
                c = str(c)
            if not c.strip():
                continue
            if is_noise_candidate(c):
                continue
            filtered_original.append(c)
        # ä½¿ç”¨ normalize_candidate åš key å»é‡ï¼Œä½†ä¿ç•™ç¬¬ä¸€å‡ºç°çš„åŸå§‹å½¢å¼
        seen = set()
        deduped: list[str] = []
        for t in filtered_original:
            key_norm = normalize_candidate(t)
            if not key_norm:
                continue
            if key_norm in seen:
                continue
            seen.add(key_norm)
            deduped.append(t)
        terms = deduped

    if not terms:
        state_dict["selected_terms"] = []
        return _rewrap(original, parent, key, state_dict)

    if len(terms) <= _MAX_TERMS_TO_PROCESS:
        state_dict["selected_terms"] = terms
        return _rewrap(original, parent, key, state_dict)

    # æ„å»º LLM prompt
    sel_prompt = f"""
ä½ æ˜¯æœ¯è¯­ç­›é€‰åŠ©æ‰‹ã€‚ä¸‹é¢ç»™å‡ºè‹¥å¹²å€™é€‰æœ¯è¯­ï¼Œè¯·ä»ä¸­é€‰å‡ºæœ€æ ¸å¿ƒçš„æœ¯è¯­ï¼Œè¦æ±‚å¿…é¡»æ˜¯ä¸å®¹æ˜“ç¿»è¯‘æˆ–è€…å¤šä¹‰çš„ï¼Œå¾ˆå®¹æ˜“ç¿»è¯‘çš„ï¼ŒåŸºç¡€çš„åŠ¡å¿…æ’é™¤ã€‚ä¸è¶…è¿‡{_MAX_TERMS_TO_PROCESS}ä¸ªã€‚æŒ‰é‡è¦æ€§æ’åºå¹¶åªè¿”å› JSON æ•°ç»„, å¦‚:["æœ¯è¯­1","æœ¯è¯­2",...]
ä¸»é¢˜: {topic}
å€™é€‰:
"""
    for t in terms:
        sel_prompt += f"\n- {t}"

    raw = None
    chosen_norm_keys: typing.Optional[list[str]] = None
    for attempt in range(1, _LLM_RETRIES + 1):
        try:
            raw = llm_completion(sel_prompt, system="ä½ æ˜¯æœ¯è¯­ç­›é€‰åŠ©æ‰‹ï¼Œä¸¥æ ¼è¿”å› JSON åˆ—è¡¨ã€‚")
            if not raw:
                time.sleep(_RETRY_BACKOFF * attempt)
                continue
            parsed = json.loads(raw)
            if isinstance(parsed, list):
                # ä½¿ç”¨ normalize_candidate åŒ¹é… LLM è¿”å›çš„æœ¯è¯­ï¼Œä½†æœ€ç»ˆä¿ç•™åŸå§‹å½¢å¼
                term_norm_to_original: dict[str, str] = {}
                for t in terms:
                    key_norm = normalize_candidate(t)
                    if key_norm and key_norm not in term_norm_to_original:
                        term_norm_to_original[key_norm] = t
                chosen_norm_keys = []
                for item in parsed:
                    if not isinstance(item, str):
                        continue
                    item_norm = normalize_candidate(item)
                    if item_norm in term_norm_to_original and item_norm not in chosen_norm_keys:
                        chosen_norm_keys.append(item_norm)
                if chosen_norm_keys:
                    break
        except Exception as _e:
            logger.debug("LLM select_top_terms å¤±è´¥ï¼ˆå°è¯• %dï¼‰: %s", attempt, _e)
            if raw:
                logger.debug("LLM raw output (select_top_terms): -----\n%s\n-----", raw)
            time.sleep(_RETRY_BACKOFF * attempt)

    if chosen_norm_keys is None:
        # å¯å‘å¼é™çº§ï¼šè¿™é‡Œä¹Ÿä¿æŒåŸå§‹å¤§å°å†™
        text_lower = state_dict.get("text", "").lower()
        scored: list[tuple[float, str]] = []
        for t in terms:
            score = 0.0
            score += len(t.split()) * 0.1
            idx = text_lower.find(t.lower())
            if idx >= 0:
                score += max(0.0, 1.0 - (idx / max(1, len(text_lower))))
            scored.append((score, t))
        scored.sort(key=lambda x: -x[0])
        # ç›´æ¥å–å‰ N ä¸ªåŸå§‹å½¢å¼
        selected_terms = [t for _, t in scored[:_MAX_TERMS_TO_PROCESS]]
    else:
        # æ ¹æ® chosen_norm_keys æ˜ å°„å›åŸå§‹å½¢å¼
        term_norm_to_original: dict[str, str] = {}
        for t in terms:
            key_norm = normalize_candidate(t)
            if key_norm and key_norm not in term_norm_to_original:
                term_norm_to_original[key_norm] = t
        selected_terms = []
        for nk in chosen_norm_keys:
            orig = term_norm_to_original.get(nk)
            if orig and orig not in selected_terms:
                selected_terms.append(orig)

    # ä¿è¯é•¿åº¦ä¸è¶…è¿‡ N
    selected_terms = selected_terms[:_MAX_TERMS_TO_PROCESS]
    state_dict["selected_terms"] = selected_terms
    return _rewrap(original, parent, key, state_dict)


@timed_node()
def finalize_translations(state: typing.Any) -> TermState:
    """åœ¨å·²ç¿»è¯‘çš„ translations_map ä¸­ä¸º state['selected_terms']ï¼ˆæˆ– state['terms']ï¼‰é€‰æ‹©æœ€ç»ˆç¿»è¯‘å¹¶ä¿å­˜ã€‚
    è¯¥èŠ‚ç‚¹ä¸å†è¿›è¡Œå¤§è§„æ¨¡ LLM top-N é€‰æ‹©ï¼Œåªè´Ÿè´£ pick + saveã€‚
    """
    original = state
    inner, parent, key = _unwrap(state)
    state = inner if isinstance(inner, dict) else {"translations": {}}
    state_dict: dict = typing.cast(dict, state)

    translations_map: Dict[str, List[str]] = state_dict.get("translations", {})
    topic = state_dict.get("topic", "")

    terms = list(state_dict.get("selected_terms") or state_dict.get("terms") or list(translations_map.keys()))
    final_translations: Dict[str, str] = {}

    def pick_best_candidate(term: str, candidates: List[str]) -> typing.Optional[str]:
        if not candidates:
            return "none"
        for c in candidates:
            if re.search(r"[\u4e00-\u9fff]", c):
                return c.strip()
        for c in candidates:
            if c and c.strip():
                return c.strip()
        return "none"

    for term in terms[:_MAX_TERMS_TO_PROCESS]:
        candidates = translations_map.get(term, [])
        chosen = pick_best_candidate(term, candidates)
        if chosen and chosen.lower() != "none":
            chosen_norm = chosen.strip()
            # ä½¿ç”¨ .lower() è¿›è¡Œä¸´æ—¶æ¯”è¾ƒ
            if term.lower() in _COMMON_GENERIC_WORDS:
                continue
            try:
                # ä½¿ç”¨ .lower() è¿›è¡Œä¸´æ—¶æ¯”è¾ƒ
                if len(term) > 1 and term.lower() not in _COMMON_GENERIC_WORDS:
                    save_translation(term, chosen_norm, "term")
            except Exception:
                logger.debug("ä¿å­˜ç¿»è¯‘æ—¶å‡ºé”™: %s -> %s", term, chosen_norm)
            final_translations[term] = chosen_norm
        else:
            final_translations[term] = "none"

    state_dict["final_translations"] = final_translations
    result = _rewrap(original, parent, key, state_dict)
    return result


# ===================== 9ï¸âƒ£ æ„å»º LangGraph å·¥ä½œæµï¼ˆé‡æ„ä¸º main.extract çš„æ‰¹å¤„ç†æµç¨‹ï¼‰ =====================
_TERMS_ONLY_WORKFLOW = None


@timed_node()
def _init_extract_state(state: typing.Any) -> dict:
    """åˆå§‹åŒ– extract æµç¨‹æ‰€éœ€å­—æ®µã€‚è¾“å…¥éœ€åŒ…å«: summary(str), chunks(dict[str,str])"""
    original = state
    inner, parent, key = _unwrap(state)
    state = inner if isinstance(inner, dict) else {}
    sd: dict = typing.cast(dict, state)
    sd.setdefault("summary", sd.get("topic", "") or "")
    sd.setdefault("chunks", {})
    sd.setdefault("per_chunk_results", [])
    sd.setdefault("unique_terms", [])
    sd.setdefault("translations_map", {})
    sd.setdefault("termAnnotations", {})
    sd.setdefault("stats", {})
    sd.setdefault("errors", [])
    return _rewrap(original, parent, key, sd)


@timed_node()
def _terms_only_batch(state: typing.Any) -> dict:
    """å¯¹æ¯ä¸ª chunk è¿è¡Œæœ¯è¯­æå–ï¼ˆä¸ç¿»è¯‘ï¼‰ï¼Œå…¼å®¹ main.extract çš„ process_chunk_terms_only è¡Œä¸ºã€‚"""
    original = state
    inner, parent, key = _unwrap(state)
    sd: dict = typing.cast(dict, inner if isinstance(inner, dict) else {})

    summary = sd.get("summary", "")
    chunks: Dict[str, str] = sd.get("chunks", {}) or {}

    global _TERMS_ONLY_WORKFLOW
    if _TERMS_ONLY_WORKFLOW is None:
        try:
            _TERMS_ONLY_WORKFLOW = build_graph_terms_only()
        except Exception as e:
            logger.exception("Failed to compile terms-only workflow in _terms_only_batch: %s", e)
            _TERMS_ONLY_WORKFLOW = None

    per_chunk_results: List[Dict[str, typing.Any]] = []
    errors: List[Dict[str, str]] = []

    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†æ¯ä¸ª chunk
    def _process_one(cid: str, ctext: typing.Any) -> Dict[str, typing.Any]:
        try:
            text = ctext if isinstance(ctext, str) else str(ctext)
            init_state = {
                "text": text,
                "candidates": [],
                "terms": [],
                "topic": summary,
                "translations": {},
                "final_translations": {},
                "term_types": {},
            }
            if _TERMS_ONLY_WORKFLOW is None:
                raise RuntimeError("terms_only_workflow_not_initialized")
            raw_res = _TERMS_ONLY_WORKFLOW.invoke(init_state)
            if isinstance(raw_res, dict):
                result = raw_res
            else:
                result = {}
            selected_terms = result.get("selected_terms") or result.get("terms") or []
            term_types = result.get("term_types", {})
            return {"chunk_id": str(cid), "terms": list(selected_terms), "term_types": dict(term_types)}
        except Exception as e:
            return {"chunk_id": str(cid), "terms": [], "term_types": {}, "error": str(e)}

    if chunks:
        max_workers = min(5, len(chunks))
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_cid = {executor.submit(_process_one, cid, ctext): str(cid) for cid, ctext in chunks.items()}
            for fut in as_completed(future_to_cid):
                r = fut.result()
                if r.get("error"):
                    errors.append({"chunk_id": r.get("chunk_id"), "error": r.get("error")})
                per_chunk_results.append(r)
    else:
        per_chunk_results = []

    sd["per_chunk_results"] = per_chunk_results
    sd.setdefault("errors", []).extend(errors)
    return _rewrap(original, parent, key, sd)


@timed_node()
def _aggregate_unique_terms(state: typing.Any) -> dict:
    original = state
    inner, parent, key = _unwrap(state)
    sd: dict = typing.cast(dict, inner if isinstance(inner, dict) else {})

    all_terms: List[str] = []
    for r in sd.get("per_chunk_results", []):
        all_terms.extend(r.get("terms", []))
    unique_terms = sorted(set(t for t in all_terms if isinstance(t, str) and t.strip()))
    sd["unique_terms"] = unique_terms
    return _rewrap(original, parent, key, sd)


@timed_node()
def _batch_translate_with_fallback(state: typing.Any) -> dict:
    original = state
    inner, parent, key = _unwrap(state)
    sd: dict = typing.cast(dict, inner if isinstance(inner, dict) else {})

    unique_terms: List[str] = sd.get("unique_terms", [])
    translations_map: Dict[str, List[str]] = {}
    topic = sd.get("summary", "")

    # æ‰¹é‡ç¿»è¯‘ï¼ˆä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†åˆ†æ‰¹ï¼‰
    try:
        if unique_terms:
            batch_size = 50
            batches = [unique_terms[i:i + batch_size] for i in range(0, len(unique_terms), batch_size)]
            max_workers = min(5, len(batches))
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                future_to_idx = {
                    executor.submit(get_translation_candidates_batch, batch, batch_size=batch_size, topic=topic): idx
                    for idx, batch in enumerate(batches)
                }
                for fut in as_completed(future_to_idx):
                    idx = future_to_idx[fut]
                    try:
                        res = fut.result() or {}
                        if isinstance(res, dict):
                            translations_map.update(res)
                        else:
                            logger.warning("Batch %d returned non-dict result, ignored", idx)
                    except Exception as e:
                        logger.warning("Batch translation failed for batch %d: %s", idx, e)
    except Exception as e:
        logger.warning("Batch translation failed, will fallback to single + db: %s", e)
        translations_map = {}

    # å•é¡¹å›é€€ + DB å›é€€
    for t in unique_terms:
        if translations_map.get(t):
            continue
        try:
            single = translate_term_external(t, topic=topic) or []
        except Exception:
            single = []
        if not single:
            try:
                local = query_term_translation(t) or []
                single = local
            except Exception:
                pass
        translations_map[t] = single or []

    sd["translations_map"] = translations_map
    return _rewrap(original, parent, key, sd)
@timed_node()
def _single_translate_concurrent(state: typing.Any) -> dict:
    original = state
    inner, parent, key = _unwrap(state)
    sd: dict = typing.cast(dict, inner if isinstance(inner, dict) else {})

    unique_terms: List[str] = sd.get("unique_terms", [])
    topic = sd.get("summary", "")
    translations_map: Dict[str, List[str]] = {}

    if not unique_terms:
        sd["translations_map"] = translations_map
        return _rewrap(original, parent, key, sd)

    # å¯ç”¨å¹¶å‘æ•°ï¼ˆä½ å¯æ ¹æ® RPM è°ƒæ•´ï¼‰
    max_workers = 50

    def worker(term: str):
        """
        å•ä¸ªè¯ç¿»è¯‘ä»»åŠ¡ï¼Œè‡ªåŠ¨ç½‘ç»œé‡è¯• 3 æ¬¡ã€‚
        å¤±è´¥æˆ–æœªæ‰¾åˆ°ç¿»è¯‘ â†’ è¿”å›ç©ºåˆ—è¡¨ã€‚
        """
        retry_delays = [0.3, 0.6, 1.0]  # æ¸è¿›å»¶è¿Ÿ

        for attempt in range(3):
            try:
                res = translate_term(term, topic=topic)
                if res:
                    return term, res
                else:
                    # æ‰¾ä¸åˆ°ç¿»è¯‘ä¸æ˜¯ç½‘ç»œé”™è¯¯ï¼Œä¸å¿…é‡è¯•
                    return term, []
            except Exception as e:
                logger.warning(
                    "Translate attempt %d failed for %s: %s",
                    attempt + 1, term, e
                )
                if attempt < 2:
                    time.sleep(retry_delays[attempt])

        # ä¸‰æ¬¡éƒ½å¤±è´¥ â†’ ç»™ç©ºåˆ—è¡¨
        return term, []

    try:
        from concurrent.futures import ThreadPoolExecutor, as_completed
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_term = {
                executor.submit(worker, t): t for t in unique_terms
            }

            for fut in as_completed(future_to_term):
                term = future_to_term[fut]
                try:
                    k, v = fut.result()
                    translations_map[k] = v
                except Exception as e:
                    logger.warning("Unexpected translation failure for %s: %s", term, e)
                    translations_map[term] = []

    except Exception as e:
        logger.exception("Concurrent translation failed: %s", e)
        translations_map = {t: [] for t in unique_terms}

    sd["translations_map"] = translations_map
    return _rewrap(original, parent, key, sd)


@timed_node()
def _assemble_annotations(state: typing.Any) -> dict:
    original = state
    inner, parent, key = _unwrap(state)
    sd: dict = typing.cast(dict, inner if isinstance(inner, dict) else {})

    chunks: Dict[str, str] = sd.get("chunks", {}) or {}
    per_chunk_results: List[Dict[str, typing.Any]] = sd.get("per_chunk_results", [])
    translations_map: Dict[str, List[str]] = sd.get("translations_map", {})

    per_chunk_map: Dict[str, Dict[str, typing.Any]] = {str(r.get("chunk_id")): r for r in per_chunk_results if r and r.get("chunk_id") is not None}

    term_annotations: Dict[str, typing.Any] = {}
    translated_count = 0

    def _pick_best(term: str, cands: List[str]) -> str:
        if not cands:
            return "none"
        # å¿½ç•¥å¤§å°å†™å’Œç©ºæ ¼åä¸åŸè¯ç›¸åŒçš„å€™é€‰
        term_norm = term.strip().lower()
        for c in cands:
            if not c:
                continue
            c_norm = c.strip()
            # å¦‚æœå€™é€‰è¯ä¸åŸè¯ç›¸åŒï¼ˆå¿½ç•¥å¤§å°å†™ï¼‰ï¼Œåˆ™è·³è¿‡
            if c_norm.lower() == term_norm:
                continue
            # ä¼˜å…ˆè¿”å›åŒ…å«ä¸­æ–‡çš„ç¿»è¯‘
            if re.search(r"[\u4e00-\u9fff]", c_norm):
                return c_norm
        # å¦‚æœæ²¡æœ‰ä¸­æ–‡ç¿»è¯‘ï¼Œä½†æœ‰å…¶ä»–ä¸åŒäºåŸæ–‡çš„ç¿»è¯‘
        for c in cands:
            if not c:
                continue
            c_norm = c.strip()
            if c_norm.lower() != term_norm:
                return c_norm
        # æ‰€æœ‰å€™é€‰éƒ½ä¸åŸè¯ç›¸åŒæˆ–ä¸ºç©º
        return "none"

    for idx, cid in enumerate(chunks.keys(), start=1):
        r = per_chunk_map.get(str(cid), {"terms": [], "term_types": {}})
        items: List[Dict[str, typing.Any]] = []
        for t in r.get("terms", []):
            # è¿™é‡Œ t æ˜¯åœ¨ terms_only_workflow ä¸­äº§ç”Ÿçš„ï¼Œå·²ä¿æŒåŸå§‹å¤§å°å†™
            cands = translations_map.get(t, [])
            chosen = _pick_best(t, cands)
            if chosen and chosen.lower() != "none":
                translated_count += 1
            # term å­—æ®µç›´æ¥ä½¿ç”¨ tï¼Œä¸åš lower å¤„ç†
            items.append({"term": t, "translation": chosen})
        term_annotations[str(idx)] = items

    stats = {
        "total_chunks": len(chunks),
        "unique_terms": len(sd.get("unique_terms", [])),
        "translated_terms": translated_count,
    }

    sd["termAnnotations"] = term_annotations
    sd["stats"] = stats
    return _rewrap(original, parent, key, sd)


@timed_node()
def build_graph():
    """æ„å»ºä¸ main.extract ç­‰ä»·çš„æ‰¹å¤„ç†å›¾ã€‚

    è¾“å…¥çŠ¶æ€éœ€è¦åŒ…å«ï¼š
    - summary: Optional[str]
    - chunks: Dict[str, str]

    è¾“å‡ºï¼ˆå†™å…¥çŠ¶æ€ï¼‰ï¼š
    - termAnnotations: Dict[str, Any]
    - stats: Dict[str, Any]
    - errors: List[Dict[str,str]]ï¼ˆè‹¥å­˜åœ¨ï¼‰
    - ä»¥åŠä¸­é—´ç»“æœï¼šper_chunk_results, unique_terms, translations_map
    """
    graph = StateGraph(dict)  # type: ignore
    graph.add_node("extract_candidates", extract_candidates)  # type: ignore
    # æ–°å¢å…ˆé€‰æ‹© top-N çš„èŠ‚ç‚¹
    graph.add_node("select_top_terms", select_top_terms)  # type: ignore

    # æœ€ç»ˆç¿»è¯‘é€‰æ‹©å™¨
    graph.add_node("finalize_translations", finalize_translations)  # type: ignore
    graph.add_node("init", _init_extract_state)  # type: ignore
    graph.add_node("terms_only_batch", _terms_only_batch)  # type: ignore
    graph.add_node("aggregate_unique_terms", _aggregate_unique_terms)  # type: ignore
    graph.add_node("batch_translate", _single_translate_concurrent)  # type: ignore
    graph.add_node("assemble_annotations", _assemble_annotations)  # type: ignore

    graph.set_entry_point("init")
    graph.add_edge("init", "terms_only_batch")
    graph.add_edge("terms_only_batch", "aggregate_unique_terms")
    graph.add_edge("aggregate_unique_terms", "batch_translate")
    graph.add_edge("batch_translate", "assemble_annotations")
    graph.add_edge("assemble_annotations", END)
    return graph.compile()
# ===================== æ–°å¢ï¼šä»…æ‰§è¡Œæœ¯è¯­æå–ä¸ç­›é€‰ï¼Œä¸åšç¿»è¯‘çš„å·¥ä½œæµ =====================
@timed_node()
def build_graph_terms_only():
    """ä»…æ‰§è¡Œæœ¯è¯­æå–ä¸ç­›é€‰ï¼Œä¸åšç¿»è¯‘ï¼Œä¾¿äºåç½®æ‰¹é‡ç¿»è¯‘ã€‚"""
    graph = StateGraph(dict)  # type: ignore
    graph.add_node("extract_candidates", extract_candidates)  # type: ignore
    # ç›´æ¥ä½¿ç”¨ select_top_termsï¼Œä¸å†ä¾èµ– filter_terms
    graph.add_node("select_top_terms", select_top_terms)  # type: ignore
    graph.set_entry_point("extract_candidates")
    graph.add_edge("extract_candidates", "select_top_terms")
    graph.add_edge("select_top_terms", END)
    return graph.compile()

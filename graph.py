from typing import List, TypedDict, Dict, Set
from langgraph.graph import StateGraph, END
import re
import spacy
import json
import time
import logging
import itertools
from functools import wraps
from utils.zhipu_client import client
from utils.db_interface import query_term_translation, save_translation
from utils.web_fetcher import fetch_core_translations
from utils.Get_term import translate_term as translate_term_external
import typing


# ç®€å•çš„åŒ…è£…/è§£åŒ…è¾…åŠ©ï¼ˆå…¼å®¹å·¥ä½œæµå¯èƒ½ä¼ å…¥çš„ä¸‰å…ƒç»„åŒ…è£…æˆ–ç›´æ¥ dictï¼‰
def _unwrap(raw: typing.Any):
    """è§£åŒ…çŠ¶æ€ï¼šè‹¥ state æ˜¯ä¸‰å…ƒç»„ (inner, parent, key) åˆ™è¿”å›å®ƒï¼Œå¦åˆ™è‹¥ä¸º dict åˆ™è¿”å› (dict, None, None)ã€‚"""
    try:
        if isinstance(raw, tuple) and len(raw) == 3:
            return raw
    except Exception:
        pass
    if isinstance(raw, dict):
        return raw, None, None
    return raw, None, None


def _rewrap(original_raw: typing.Any, parent, key, new_inner: typing.MutableMapping):
    """å°† new_inner åŒ…å›ï¼šè‹¥ original_raw ä¸ºä¸‰å…ƒç»„åˆ™è¿”å› (new_inner, parent, key)ï¼Œå¦åˆ™è¿”å› new_inner æœ¬èº«ã€‚"""
    try:
        if isinstance(original_raw, tuple) and len(original_raw) == 3:
            return (new_inner, parent, key)
    except Exception:
        pass
    return new_inner


# ===================== 1ï¸âƒ£ å®šä¹‰çŠ¶æ€ç±»å‹ =====================
class TermState(TypedDict):
    text: str
    candidates: List[str]
    terms: List[str]
    topic: str
    translations: Dict[str, List[str]]  # term -> å€™é€‰ç¿»è¯‘
    final_translations: Dict[str, str]


# ===================== 2ï¸âƒ£ åˆå§‹åŒ– =====================
nlp = spacy.load("en_core_web_sm")

# logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å¯è°ƒå‚æ•°
_MAX_TERMS_TO_PROCESS = 6  # ç›®æ ‡æå– 5-6 ä¸ªæœ¯è¯­ï¼ˆæœ€å¤§å€™é€‰æ•°ï¼‰
_LLM_RETRIES = 2
_RETRY_BACKOFF = 1.0
_NOISE_MIN_CHAR = 2

# Performance tuning params
_MAX_NOUN_CHUNKS = 80
_TRANSLATE_WORKERS = 6
_TRANSLATE_TIMEOUT = 6.0  # seconds per translation task

# LLM call tuning
_LLM_TIMEOUT = 12.0  # seconds per LLM request
_LLM_CACHE_ENABLED = True

# counters
LLM_CALLS = 0
DICT_CALLS = 0

# å¸¸è§éæœ¯è¯­å™ªå£°è¯ï¼ˆå°å†™ï¼‰
_EXTRA_NOISE = {
    "that", "this", "these", "those", "it", "they", "he", "she",
    "a", "an", "the", "use", "uses", "used", "based",
}
# æ˜æ˜¾ä¸åº”å…¥åº“çš„æ™®é€šè¯ï¼ˆå¯æ‰©å±•ï¼‰
_COMMON_GENERIC_WORDS = {
    "that", "this", "an", "a", "the", "function", "input", "output",
    "example", "learning", "machine", "maps", "pairs", "task",
}


# ----------------- ç®€åŒ–ç‰ˆ helpersï¼ˆå·²ç§»é™¤ç¼“å­˜ä¸å¹¶å‘ï¼Œä¾¿äºéªŒè¯ï¼‰ -----------------
def _cached_nlp(text: str):
    """ç›´æ¥è°ƒç”¨ spaCyï¼ˆå·²ç§»é™¤ç¼“å­˜ï¼‰ã€‚"""
    return nlp(text)


def _cached_translate_term_external(term: str):
    """ç›´æ¥è°ƒç”¨å¤–éƒ¨ç¿»è¯‘å™¨ï¼ˆæ— ç¼“å­˜ï¼‰ã€‚è¿”å›åˆ—è¡¨å½¢å¼ä»¥å…¼å®¹ç°æœ‰ä»£ç ã€‚"""
    try:
        res = translate_term_external(term) or []
        return list(res)
    except Exception:
        return []


def _cached_fetch_core_translations(term: str):
    """ç›´æ¥è°ƒç”¨ fetch_core_translationsï¼ˆæ— ç¼“å­˜ï¼‰ã€‚"""
    try:
        res = fetch_core_translations(term) or []
        return list(res)
    except Exception:
        return []


def _cached_llm_completion(prompt: str, system: str = "ä½ æ˜¯æœ¯è¯­åˆ†ç±»åŠ©æ‰‹") -> str:
    """ç›´æ¥è°ƒç”¨ LLMï¼ŒåŒæ­¥é˜»å¡ï¼Œä¸åšç¼“å­˜æˆ–çº¿ç¨‹è¶…æ—¶æ§åˆ¶ï¼ˆä¾¿äºéªŒè¯æ­£ç¡®æ€§ï¼‰ã€‚

    æ³¨æ„ï¼šå¦‚æœä½ éœ€è¦è¶…æ—¶ä¿æŠ¤ï¼Œå¯ä»¥åœ¨åç»­æ¢å¤çº¿ç¨‹/è¶…æ—¶é€»è¾‘ã€‚
    """
    global LLM_CALLS
    LLM_CALLS += 1
    try:
        completion = client.chat.completions.create(
            model="glm-4.5",
            messages=[{"role": "system", "content": system}, {"role": "user", "content": prompt}],
            temperature=0,
        )
        return _safe_extract_completion_content(completion)
    except Exception as e:
        logger.warning("LLM direct call failed: %s", e)
        return ""


# Decorator to time node functions and log start/finish with simple state sizes
def timed_node(name: str = None):
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            node_name = name or func.__name__
            logger.info("Node %s START", node_name)
            start = time.perf_counter()
            try:
                return func(*args, **kwargs)
            finally:
                end = time.perf_counter()
                dur = end - start
                # Try to extract counts from the first arg if it's the workflow state
                try:
                    state = args[0] if args else None
                    inner, parent, key = _unwrap(state) if state is not None else (None, None, None)
                    sd = inner if isinstance(inner, dict) else (state if isinstance(state, dict) else {})
                    cands = sd.get("candidates") if isinstance(sd, dict) else None
                    terms = sd.get("terms") if isinstance(sd, dict) else None
                    trans = sd.get("translations") if isinstance(sd, dict) else None
                    logger.info("Node %s END (%.3fs) candidates=%s terms=%s translations=%s", node_name, dur,
                                (len(cands) if isinstance(cands, (list, set)) else '-' ),
                                (len(terms) if isinstance(terms, (list, set)) else '-' ),
                                (len(trans) if isinstance(trans, dict) else '-' ))
                except Exception:
                    logger.info("Node %s END (%.3fs)", node_name, dur)
        return wrapper
    return decorator


# ===================== è¾…åŠ©å‡½æ•°ï¼šæ ‡å‡†åŒ–/å™ªå£°è¿‡æ»¤/å»é‡ =====================
LEADING_ARTICLES = re.compile(r'^(?:a|an|the)\s+', flags=re.I)


def normalize_candidate(text: str) -> str:
    """
    æ›´ä¸¥æ ¼çš„æ ‡å‡†åŒ–ï¼š
    - å»é¦–å°¾ç©ºç™½ã€é¦–å† è¯ã€å»é™¤ possessive (\'s) ä¸ä¸å¿…è¦å¼•å·/æ‹¬å·
    - åˆå¹¶ç©ºæ ¼ã€å°å†™
    - è¿”å›ç©ºå­—ç¬¦ä¸²è¡¨ç¤ºè¢«å®Œå…¨ç§»é™¤
    """
    if not text:
        return ""
    s = text.strip()
    # å»æ‰å…¸å‹çš„é¦–å† è¯
    s = re.sub(r'^(?:a|an|the)\s+', '', s, flags=re.I)
    # å»æ‰ possessive " 's" ä¸å­¤ç«‹çš„å•å¼•å·ã€åŒå¼•å·ä»¥åŠåŒ…è£¹çš„æ‹¬å·
    s = re.sub(r"\'s\b", "", s, flags=re.I)
    s = s.strip(" `\"â€œâ€()[]{}")
    # åˆ é™¤ä¸¤ç«¯å¤šä½™æ ‡ç‚¹ï¼ˆä¿ç•™ä¸­é—´çš„è¿å­—ç¬¦/æ–œæ ï¼‰
    s = re.sub(r'^[.:;\-]+|[.:;\-]+$', '', s)
    s = re.sub(r'\s+', ' ', s)
    s = s.lower().strip()
    return s


def is_noise_candidate(candidate: str) -> bool:
    if not candidate:
        return True
    if candidate in _EXTRA_NOISE:
        return True
    # å¾ˆçŸ­çš„éå­—æ¯ä¸²
    if len(candidate.replace(" ", "")) <= _NOISE_MIN_CHAR:
        return True
    # å¿…é¡»åŒ…å«å­—æ¯
    if not re.search(r"[a-zA-Z]", candidate):
        return True

    # ä½¿ç”¨ spaCy è¿›ä¸€æ­¥åˆ¤æ–­ï¼šè‡³å°‘åŒ…å«ä¸€ä¸ªåè¯/ä¸“æœ‰åè¯
    try:
        doc = _cached_nlp(candidate)
    except Exception:
        doc = nlp(candidate)
    has_content = any(getattr(t, 'pos_', None) in ("NOUN", "PROPN") for t in doc)
    if not has_content:
        return True
    # é¿å…ä»…ä¸ºä»£è¯/é™å®šè¯
    all_noise = all((getattr(t, 'is_stop', False) or getattr(t, 'pos_', None) in ("PRON", "DET", "ADP", "PART", "PUNCT")) for t in doc)
    if all_noise:
        return True
    return False


def dedupe_keep_longest(candidates: List[str]) -> List[str]:
    """ä¿ç•™æœ€é•¿çŸ­è¯­ï¼Œè‹¥ A å®Œæ•´åŒ…å« Bï¼ˆè¯è¾¹ç•Œï¼‰åˆ™ä¿ç•™ Aï¼Œä¸¢å¼ƒ Bã€‚"""
    uniq = sorted(set(candidates), key=lambda x: (-len(x.split()), x))
    kept: List[str] = []
    for cand in uniq:
        cand_words_pattern = r'\b' + re.escape(cand) + r'\b'
        skip = False
        for k in kept:
            if re.search(cand_words_pattern, k):
                skip = True
                break
        if not skip:
            kept.append(cand)
    # è¿”å›ç¨³å®šæ’åºï¼ˆæŒ‰é•¿åº¦å’Œå­—æ¯ï¼‰
    return sorted(kept)


def extract_candidates(state: typing.Any) -> TermState:
    original = state
    inner, parent, key = _unwrap(state)
    state = inner if isinstance(inner, dict) else {"text": ""}
    state_dict: dict = typing.cast(dict, state)

    text = state_dict.get("text", "")
    if not text:
        logger.warning("extract_candidates received state without 'text' key")
        state_dict.setdefault("text", "")
        text = state_dict["text"]

    candidates: Set[str] = set()

    # 1ï¸âƒ£ spaCy åè¯çŸ­è¯­ï¼ˆé™åˆ¶æ•°é‡ä»¥æé«˜é€Ÿåº¦ï¼‰
    doc = _cached_nlp(text)
    for chunk in itertools.islice(doc.noun_chunks, _MAX_NOUN_CHUNKS):
        chunk_text = getattr(chunk, "text", str(chunk)).strip()
        if chunk_text:  # ä¸é™åˆ¶é•¿åº¦ï¼Œä½†é™åˆ¶æ€»æ•°
            candidates.add(chunk_text)

    # 2ï¸âƒ£ spaCy ä¸“æœ‰åè¯
    proper_nouns = {getattr(token, "text", str(token)).strip()
                    for token in doc if getattr(token, "pos_", None) == "PROPN"}
    candidates.update(proper_nouns)

    # 3ï¸âƒ£ è¿å­—ç¬¦/ä¸‹åˆ’çº¿ç»„åˆã€ç¼©å†™
    regex_terms = set(re.findall(r"\b[A-Za-z]+(?:[-_/][A-Za-z]+)+\b", text))
    candidates.update(regex_terms)

    # 4ï¸âƒ£ æ ‡å‡†åŒ– + å»å™ªï¼ˆåªå»æ‰æ˜æ˜¾æ— ç”¨çŸ­è¯ï¼‰
    normalized = []
    for c in candidates:
        c_norm = normalize_candidate(c)
        if not c_norm:
            continue
        if c_norm in _EXTRA_NOISE:
            continue
        # è‡³å°‘åŒ…å«ä¸€ä¸ªå­—æ¯
        if not re.search(r"[a-zA-Z]", c_norm):
            continue
        normalized.append(c_norm)

    # 5ï¸âƒ£ å»é‡å¤ï¼Œä¼˜å…ˆä¿ç•™æœ€é•¿çŸ­è¯­
    final_candidates = dedupe_keep_longest(normalized)

    # ä½¿ç”¨å¯å‘å¼è¯„åˆ†å¯¹å€™é€‰è¿›è¡Œæ’åºå¹¶ä¼˜å…ˆé€‰å– top Nï¼ˆæ›¿ä»£ KeyBERTï¼‰
    def score_candidate(cand: str) -> float:
        s = 0.0
        cand_l = cand.lower()
        text_l = text.lower()
        # å‡ºç°é¢‘æ¬¡ï¼ˆæ›´é¢‘ç¹ä¼˜å…ˆï¼‰
        try:
            occ = text_l.count(cand_l)
        except Exception:
            occ = 0
        s += occ * 2.0
        # è¯æ•°ï¼ˆæ›´é•¿çŸ­è¯­ç¨å¾®åŠ åˆ†ï¼‰
        words = len(cand.split())
        s += 0.5 * words
        # è¯æ€§è¯„åˆ†ï¼šå°½é‡å¤ç”¨åŸå§‹ doc çš„å­—ç¬¦è·¨åº¦ä»¥é¿å…é‡å¤è§£æ
        pos_score = 0
        start_idx = text_l.find(cand_l)
        if start_idx >= 0:
            span = doc.char_span(start_idx, start_idx + len(cand_l), alignment_mode='expand')
            if span is not None:
                for t in span:
                    if getattr(t, 'pos_', None) == 'PROPN':
                        pos_score += 2
                    elif getattr(t, 'pos_', None) == 'NOUN':
                        pos_score += 1
                    elif getattr(t, 'pos_', None) == 'ADJ':
                        pos_score += 0.3
        else:
            # å›é€€åˆ°ç¼“å­˜çš„ nlp åˆ†æ
            try:
                docc = _cached_nlp(cand)
                for t in docc:
                    if getattr(t, 'pos_', None) == 'PROPN':
                        pos_score += 2
                    elif getattr(t, 'pos_', None) == 'NOUN':
                        pos_score += 1
                    elif getattr(t, 'pos_', None) == 'ADJ':
                        pos_score += 0.3
            except Exception:
                pass
        s += pos_score

        # åŒ…å«è¿å­—ç¬¦/ä¸‹åˆ’çº¿è§†ä¸ºæŠ€æœ¯çŸ­è¯­ï¼ŒåŠ åˆ†
        if re.search(r"[-_/]", cand):
            s += 1.0
        # å‡ºç°ä½ç½®ï¼šè¶Šé å‰è¶Šå¥½
        idx = text_l.find(cand_l)
        if idx >= 0:
            pos_bonus = max(0.0, 1.0 - (idx / max(1, len(text_l))))
            s += pos_bonus
        # é•¿åº¦å½’ä¸€åŒ–å°åŠ åˆ†
        s += min(len(cand), 50) / 50.0
        return s

    scored = []
    for c in final_candidates:
        if is_noise_candidate(c):
            continue
        scored.append((score_candidate(c), c))
    # æŒ‰åˆ†æ•°é™åºï¼Œåˆ†æ•°ç›¸åŒæ—¶æŒ‰çŸ­è¯­é•¿åº¦é™åº
    scored.sort(key=lambda x: (-x[0], -len(x[1].split())))
    final_candidates = [c for _, c in scored][:_MAX_TERMS_TO_PROCESS]

    # 6ï¸âƒ£ é™åˆ¶æ•°é‡ï¼Œé¿å…åç»­å¤§é‡ LLM è°ƒç”¨
    if len(final_candidates) > _MAX_TERMS_TO_PROCESS:
        logger.info("å€™é€‰æœ¯è¯­è¿‡å¤š(%d)ï¼Œæˆªæ–­åˆ° %d", len(final_candidates), _MAX_TERMS_TO_PROCESS)
        final_candidates = final_candidates[:_MAX_TERMS_TO_PROCESS]

    state_dict["candidates"] = sorted(final_candidates)
    result = _rewrap(original, parent, key, state_dict)
    return result


# ===================== 4ï¸âƒ£ æœ¯è¯­ç­›é€‰ï¼ˆè°ƒç”¨ LLM åå†æ¸…æ´—ï¼‰ =====================
@timed_node()
def filter_terms(state: typing.Any) -> TermState:
    original = state
    inner, parent, key = _unwrap(state)
    state = inner if isinstance(inner, dict) else {"candidates": []}
    state_dict: dict = typing.cast(dict, state)

    # quick local fallback: if very few candidates, avoid LLM and use rules
    cands = state_dict.get("candidates", [])
    if not cands:
        state_dict["terms"] = []
        state_dict.setdefault("term_types", {})
        return _rewrap(original, parent, key, state_dict)

    if len(cands) <= 2:
        filtered = []
        for c in cands:
            c_norm = normalize_candidate(c)
            if not c_norm:
                continue
            if is_noise_candidate(c_norm):
                continue
            filtered.append(c_norm)
        state_dict["terms"] = sorted(set(filtered))
        # ç®€å•æƒ…å†µå…¨éƒ¨è§†ä¸º term
        state_dict["term_types"] = {t: "term" for t in state_dict["terms"]}
        return _rewrap(original, parent, key, state_dict)

    prompt = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šæœ¯è¯­è¯†åˆ«åŠ©æ‰‹ã€‚ä¸‹é¢ç»™å‡ºä¸€ä¸ªå€™é€‰æœ¯è¯­åˆ—è¡¨ï¼ˆæ¥è‡ªæ–‡æ¡£è‡ªåŠ¨æŠ½å–ï¼‰ã€‚è¯·ä¸¥æ ¼ä»è¯¥å€™é€‰åˆ—è¡¨ä¸­æŒ‘é€‰å¹¶åˆ†ç±»ï¼Œè¿”å›ä¸¥æ ¼çš„ JSON å¯¹è±¡ï¼ˆä»…æ­¤è¾“å‡ºï¼Œç»å¯¹ä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæ€§æ–‡å­—ã€æ³¨é‡Šæˆ–æ¢è¡Œä¹‹å¤–çš„å†…å®¹ï¼‰ã€‚è¾“å‡ºæ ¼å¼å¿…é¡»æ˜¯ä¸€ä¸ª JSON å¯¹è±¡ï¼ŒåŒ…å«ä¸¤ä¸ªé”®ï¼š "term" å’Œ "proper_noun"ï¼Œå®ƒä»¬å¯¹åº”çš„å€¼éƒ½æ˜¯å­—ç¬¦ä¸²æ•°ç»„ã€‚ä¾‹å¦‚ï¼š{"term":["æœ¯è¯­A","æœ¯è¯­B"],"proper_noun":["ä¸“æœ‰å1"]}
çº¦æŸä¸è§„åˆ™ï¼ˆåŠ¡å¿…éµå®ˆï¼‰ï¼š
åªä»å€™é€‰åˆ—è¡¨ä¸­é€‰æ‹©å€™é€‰é¡¹ï¼Œä¸”å¿…é¡»æŒ‰å€™é€‰åˆ—è¡¨ä¸­çš„åŸæ ·æ–‡æœ¬è¿”å›ï¼ˆä¸è¦æ”¹å†™å€™é€‰æ–‡æœ¬çš„å­—é¢å½¢å¼ï¼‰ã€‚ä¸è¦åˆ›é€ æ–°æœ¯è¯­æˆ–æ‹¼å†™å˜ä½“ï¼›å¦‚æœå€™é€‰é‡Œå­˜åœ¨åŒä¹‰æˆ–é‡å¤é¡¹åªä¿ç•™ä¸€æ¬¡ã€‚
åˆ†ç±»è¯´æ˜ï¼š
"term"ï¼šå­¦æœ¯/æŠ€æœ¯æœ¯è¯­ã€ç†è®ºæ¦‚å¿µå¸¸è§çš„å­¦æœ¯/å·¥ç¨‹æœ¯è¯­
"proper_noun"ï¼šä¸“æœ‰åè¯ã€ç®—æ³•åã€æ¨¡å‹åã€åº“/æ¡†æ¶åã€æ•°æ®é›†åç§°ã€å…¬å¸/ç»„ç»‡åã€äº§å“åã€æ˜ç¡®çš„ç¼©å†™æˆ–é¦–å­—æ¯ç¼©å†™
é•¿åº¦é™åˆ¶ï¼šæ‰€é€‰çŸ­è¯­é•¿åº¦ï¼ˆä»¥è¯ä¸ºå•ä½ï¼‰ä¸åº”è¶…è¿‡ 4â€“5 ä¸ªå•è¯ã€‚è¶…è¿‡è¯¥é•¿åº¦çš„å€™é€‰è¯·æ’é™¤ï¼Œé™¤éå®ƒæ˜æ˜¾ä¸ºä¸€ä¸ªå·²å‘½åçš„ä¸“æœ‰åï¼ˆä»åº”æ”¾åœ¨ proper_noun ä¸­ï¼‰ã€‚
ä¸¥æ ¼æ’é™¤ï¼šä¸è¦é€‰æ‹©æ˜æ˜¾çš„æ™®é€šè¯æˆ–æ— æ„ä¹‰çŸ­è¯­ï¼Œä¾‹å¦‚ "data", "set", "vector", "each example", "the method", "this paper" ç­‰ã€‚è‹¥å€™é€‰ä»…æ˜¯åœç”¨è¯/ä»£è¯/çŸ­æ³›è¯ï¼Œåº”æ’é™¤ã€‚
æ•°é‡ä¸å»é‡ï¼šæ¯ä¸ªåˆ†ç±»å†…éƒ¨å»é‡ï¼ˆä¸é‡å¤è¿”å›åŒä¸€å­—ç¬¦ä¸²ï¼‰
è¾“å‡ºè¦æ±‚ï¼š
å¿…é¡»è¿”å›åˆæ³•å¯ parse çš„ JSONï¼Œä»…æ­¤ä¸€è¡Œæˆ–ç´§å‡‘ JSONï¼ˆä¸å…è®¸å¤šè¡Œæ–‡æœ¬/äººç±»è¯´æ˜ï¼‰ã€‚
"""
    try:
        cands_repr = json.dumps(state_dict.get('candidates', []), ensure_ascii=False)
    except Exception:
        cands_repr = str(state_dict.get('candidates', []))
    prompt += "\nå€™é€‰è¯åˆ—è¡¨:\n" + cands_repr + "\n"

    terms = []
    proper_nouns = []
    term_types: Dict[str, str] = {}
    try:
        raw_text = _cached_llm_completion(prompt, system="ä½ æ˜¯æœ¯è¯­åˆ†ç±»åŠ©æ‰‹")
        if not raw_text:
            raise ValueError("empty LLM response or timed out")
        parsed = json.loads(raw_text)
        terms = list(set(parsed.get("term", [])))
        proper_nouns = list(set(parsed.get("proper_noun", [])))
    except Exception as e:
        logger.warning("LLM åˆ†ç±»å¤±è´¥æˆ–è¿”å›é JSONï¼Œé™çº§è¿‡æ»¤: %s", e)
        candidates = state_dict.get("candidates", [])
        filtered = []
        for c in candidates:
            if is_noise_candidate(c):
                continue
            dd = _cached_nlp(c)
            if any(getattr(t, 'pos_', None) in ("NOUN", "PROPN") for t in dd):
                filtered.append(c)
        terms = dedupe_keep_longest(filtered)
        proper_nouns = []

    final_terms = []
    for t in set(terms + proper_nouns):
        t_norm = normalize_candidate(t)
        if not t_norm or is_noise_candidate(t_norm):
            continue
        final_terms.append(t_norm)
        if t in proper_nouns:
            term_types[t_norm] = "proper_noun"
        else:
            term_types[t_norm] = "term"

    state_dict["terms"] = sorted(set(final_terms))
    state_dict["term_types"] = term_types
    result = _rewrap(original, parent, key, state_dict)
    return result


# ===================== 5ï¸âƒ£ åˆ¤æ–­æŠ“å–æ–¹å¼ =====================
def decide_post_db_method(term: str) -> str:
    term_lower = term.lower()
    simple_words = {"data", "set", "map", "input", "output", "value", "use", "function"}
    if term_lower in simple_words or len(term) <= 3:
        return "dict"
    if len(term.split()) > 1 or re.search(r"[-_/]", term):
        return "llm"
    return "dict"


# ===================== 6ï¸âƒ£ è·å–å€™é€‰ç¿»è¯‘ï¼ˆå¸¦ç¼“å­˜ä¸é‡è¯•ï¼‰ =====================
def _safe_extract_completion_content(completion) -> str:
    """Robustly extract text/content from various completion response shapes.
    Returns empty string when content is missing or blank.
    """
    if not completion:
        return ""
    try:
        # common OpenAI-like shape
        choices = None
        if isinstance(completion, dict):
            choices = completion.get('choices')
        else:
            choices = getattr(completion, 'choices', None)
        if choices:
            first = choices[0]
            if isinstance(first, dict):
                msg = first.get('message') or first.get('text')
            else:
                msg = getattr(first, 'message', None) or getattr(first, 'text', None)
            if not msg:
                return ""
            if isinstance(msg, dict):
                return (msg.get('content') or msg.get('text') or "").strip()
            else:
                return str(getattr(msg, 'content', None) or msg).strip()
        # fallback for direct attribute
        if isinstance(completion, dict):
            return (completion.get('text') or "").strip()
        return (getattr(completion, 'text', '') or '').strip()
    except Exception:
        return ""


def _fetch_core_translations_with_retry(term: str, retries=2, backoff=1.0):
    global DICT_CALLS
    attempt = 0
    while attempt <= retries:
        attempt += 1
        try:
            DICT_CALLS += 1
            # use cached wrapper to avoid repeated network calls
            return list(_cached_fetch_core_translations(term))
        except Exception as e:
            logger.warning("fetch_core_translations attempt %d failed for %s: %s", attempt, term, e)
            if attempt > retries:
                return []
            time.sleep(backoff * attempt)


# ===================== 7ï¸âƒ£ ç¿»è¯‘èŠ‚ç‚¹ï¼ˆå¹¶å‘ + ç¼“å­˜ + è¶…æ—¶ï¼‰ =====================
@timed_node()
def translate_node(state: typing.Any) -> TermState:
    """Concurrent translation of detected terms with caching and timeouts.
    æ”¹ä¸ºåªç¿»è¯‘ state['selected_terms']ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼Œå¦åˆ™é€€å›åˆ° state['terms']ã€‚
    æ”¯æŒæ‰¹é‡ LLM ç¿»è¯‘ä»¥å‡å°‘è¯·æ±‚æ¬¡æ•°ã€‚
    """
    original = state
    inner, parent, key = _unwrap(state)
    state = inner if isinstance(inner, dict) else {"terms": []}
    state_dict: dict = typing.cast(dict, state)

    translations: Dict[str, List[str]] = {}
    final_translations: Dict[str, str] = {}

    # ä¼˜å…ˆä½¿ç”¨ selected_termsï¼ˆç”± select_top_terms èŠ‚ç‚¹ç”Ÿæˆï¼‰
    terms = state_dict.get("selected_terms") or state_dict.get("terms") or []
    if not terms:
        state_dict["translations"] = translations
        state_dict["final_translations"] = final_translations
        return _rewrap(original, parent, key, state_dict)

    # -----------------------------
    # ğŸ”¹ æ–°å¢ï¼šæ‰¹é‡è°ƒç”¨æœ¬åœ°ç¿»è¯‘æ¥å£ï¼ˆå« LLM æ‰¹é‡ç¿»è¯‘ï¼‰
    # -----------------------------
    try:
        from utils.Get_term import get_translation_candidates_batch
        batch_result = get_translation_candidates_batch(terms)
    except Exception as e:
        logger.warning("æ‰¹é‡ç¿»è¯‘å¤±è´¥ï¼Œå›é€€åˆ°å•é¡¹æ¨¡å¼: %s", e)
        batch_result = {}

    # -----------------------------
    # ğŸ”¹ å¯¹æ¯ä¸ª term è¿›è¡Œæ¸…ç†ã€é™çº§å›é€€
    # -----------------------------
    for term in terms:
        candidates = batch_result.get(term, [])

        # å¦‚æœæ‰¹é‡ç¿»è¯‘ä¸ºç©ºï¼Œåˆ™å°è¯•æ—§é€»è¾‘ï¼ˆç¼“å­˜æˆ– web æŠ“å–ï¼‰
        if not candidates:
            try:
                from utils.Get_term import translate_term as translate_term_external
                candidates = translate_term_external(term)
            except Exception as e:
                logger.warning("å•é¡¹å›é€€ç¿»è¯‘å¤±è´¥ %s: %s", term, e)
                candidates = []

        # clean + dedupe preserving order
        seen = set()
        cleaned = []
        for c in candidates:
            if not c:
                continue
            c_clean = str(c).strip()
            if not c_clean or c_clean in seen:
                continue
            seen.add(c_clean)
            cleaned.append(c_clean)

        translations[term] = cleaned
        if cleaned:
            final_translations[term] = cleaned[0]
        else:
            try:
                local = query_term_translation(term) or []
                if local:
                    translations[term] = local
                    final_translations[term] = local[0]
            except Exception:
                pass

    state_dict["translations"] = translations
    state_dict["final_translations"] = final_translations
    result = _rewrap(original, parent, key, state_dict)
    return result



# ===================== æ–°å¢èŠ‚ç‚¹ï¼šå…ˆé€‰å‡ºæœ€ç»ˆè¦ç¿»è¯‘çš„ top-N æœ¯è¯­ =====================
@timed_node()
def select_top_terms(state: typing.Any) -> TermState:
    """ä» state['terms'] ä¸­é€‰å‡ºæœ€å¤š _MAX_TERMS_TO_PROCESS ä¸ªé«˜è´¨é‡æœ¯è¯­ï¼Œè¾“å‡ºåˆ° state['selected_terms']ã€‚

    ä¼˜å…ˆä½¿ç”¨ LLMï¼›å¤±è´¥æ—¶ä½¿ç”¨å¯å‘å¼é™çº§ã€‚
    """
    original = state
    inner, parent, key = _unwrap(state)
    state = inner if isinstance(inner, dict) else {"terms": []}
    state_dict: dict = typing.cast(dict, state)

    terms = list(state_dict.get("terms", []) or [])
    topic = state_dict.get("topic", "")

    if not terms:
        state_dict["selected_terms"] = []
        return _rewrap(original, parent, key, state_dict)

    if len(terms) <= _MAX_TERMS_TO_PROCESS:
        state_dict["selected_terms"] = terms
        return _rewrap(original, parent, key, state_dict)

    # æ„å»º LLM promptï¼Œè¦æ±‚è¿”å› JSON åˆ—è¡¨: ["term1","term2",...]
    sel_prompt = f"""
ä½ æ˜¯æœ¯è¯­ç­›é€‰åŠ©æ‰‹ã€‚ä¸‹é¢ç»™å‡ºè‹¥å¹²å€™é€‰æœ¯è¯­ï¼Œè¯·ä»ä¸­é€‰å‡ºæœ€æ ¸å¿ƒçš„ {_MAX_TERMS_TO_PROCESS} ä¸ªæœ¯è¯­ï¼ŒæŒ‰é‡è¦æ€§æ’åºå¹¶åªè¿”å› JSON æ•°ç»„, å¦‚:["æœ¯è¯­1","æœ¯è¯­2",...]
ä¸»é¢˜: {topic}
å€™é€‰:
"""
    for t in terms:
        sel_prompt += f"\n- {t}"

    raw = None
    chosen_list = None
    for attempt in range(1, _LLM_RETRIES + 1):
        try:
            raw = _cached_llm_completion(sel_prompt, system="ä½ æ˜¯æœ¯è¯­ç­›é€‰åŠ©æ‰‹ï¼Œä¸¥æ ¼è¿”å› JSON åˆ—è¡¨ã€‚")
            if not raw:
                time.sleep(_RETRY_BACKOFF * attempt)
                continue
            parsed = json.loads(raw)
            if isinstance(parsed, list):
                # å½’ä¸€åŒ–å¹¶åªä¿ç•™åœ¨åŸå§‹ terms åˆ—è¡¨ä¸­çš„é¡¹
                chosen_list = []
                term_norm_set = {normalize_candidate(x): x for x in terms}
                for item in parsed:
                    if not isinstance(item, str):
                        continue
                    item_norm = normalize_candidate(item)
                    if item_norm in term_norm_set and item_norm not in chosen_list:
                        chosen_list.append(item_norm)
                if chosen_list:
                    break
        except Exception as _e:
            logger.debug("LLM select_top_terms å¤±è´¥ï¼ˆå°è¯• %dï¼‰: %s", attempt, _e)
            if raw:
                logger.debug("LLM raw output (select_top_terms): -----\n%s\n-----", raw)
            time.sleep(_RETRY_BACKOFF * attempt)

    if chosen_list is None:
        # é™çº§ï¼šå¯å‘å¼é€‰æ‹©ï¼ŒæŒ‰å‡ºç°ä½ç½®å’Œè¯æ•°æ’åº
        text = state_dict.get("text", "").lower()
        scored = []
        for t in terms:
            score = 0
            score += len(t.split()) * 0.1
            idx = text.find(t.lower())
            if idx >= 0:
                score += max(0.0, 1.0 - (idx / max(1, len(text))))
            scored.append((score, t))
        scored.sort(key=lambda x: -x[0])
        chosen_list = [normalize_candidate(t) for _, t in scored[:_MAX_TERMS_TO_PROCESS]]

    # ä¿è¯é•¿åº¦ä¸è¶…è¿‡ N
    chosen_list = list(dict.fromkeys(chosen_list))[:_MAX_TERMS_TO_PROCESS]

    state_dict["selected_terms"] = chosen_list
    result = _rewrap(original, parent, key, state_dict)
    return result


# ===================== é‡æ„ï¼šåªè´Ÿè´£ä» translations ä¸­é€‰æ‹©æœ€ç»ˆç¿»è¯‘å¹¶ä¿å­˜ =====================
@timed_node()
def finalize_translations(state: typing.Any) -> TermState:
    """åœ¨å·²ç¿»è¯‘çš„ translations_map ä¸­ä¸º state['selected_terms']ï¼ˆæˆ– state['terms']ï¼‰é€‰æ‹©æœ€ç»ˆç¿»è¯‘å¹¶ä¿å­˜ã€‚
    è¯¥èŠ‚ç‚¹ä¸å†è¿›è¡Œå¤§è§„æ¨¡ LLM top-N é€‰æ‹©ï¼Œåªè´Ÿè´£ pick + saveã€‚
    """
    original = state
    inner, parent, key = _unwrap(state)
    state = inner if isinstance(inner, dict) else {"translations": {}}
    state_dict: dict = typing.cast(dict, state)

    translations_map: Dict[str, List[str]] = state_dict.get("translations", {})
    topic = state_dict.get("topic", "")

    terms = list(state_dict.get("selected_terms") or state_dict.get("terms") or list(translations_map.keys()))
    final_translations: Dict[str, str] = {}

    def pick_best_candidate(term: str, candidates: List[str]) -> typing.Optional[str]:
        if not candidates:
            return None
        for c in candidates:
            if re.search(r"[\u4e00-\u9fff]", c):
                return c.strip()
        for c in candidates:
            if c and c.strip():
                return c.strip()
        return None

    for term in terms[:_MAX_TERMS_TO_PROCESS]:
        candidates = translations_map.get(term, [])
        chosen = pick_best_candidate(term, candidates)
        if chosen:
            chosen_norm = chosen.strip()
            if normalize_candidate(term) in _COMMON_GENERIC_WORDS:
                continue
            try:
                if len(term) > 1 and normalize_candidate(term) not in _COMMON_GENERIC_WORDS:
                    save_translation(term, chosen_norm, "term")
            except Exception:
                logger.debug("ä¿å­˜ç¿»è¯‘æ—¶å‡ºé”™: %s -> %s", term, chosen_norm)
            final_translations[term] = chosen_norm

    state_dict["final_translations"] = final_translations
    result = _rewrap(original, parent, key, state_dict)
    return result


# ===================== 9ï¸âƒ£ æ„å»º LangGraph å·¥ä½œæµ =====================
@timed_node()
def build_graph():
    # Use a generic mapping type for the graph's input type to satisfy type checkers
    graph = StateGraph(dict)  # type: ignore
    graph.add_node("extract_candidates", extract_candidates)  # type: ignore
    graph.add_node("filter_terms", filter_terms)  # type: ignore
    # æ–°å¢å…ˆé€‰æ‹© top-N çš„èŠ‚ç‚¹
    graph.add_node("select_top_terms", select_top_terms)  # type: ignore
    # ç¿»è¯‘åªä½œç”¨äº selected_terms
    graph.add_node("translate_node", translate_node)  # type: ignore
    # æœ€ç»ˆç¿»è¯‘é€‰æ‹©å™¨
    graph.add_node("finalize_translations", finalize_translations)  # type: ignore

    graph.set_entry_point("extract_candidates")
    graph.add_edge("extract_candidates", "filter_terms")
    graph.add_edge("filter_terms", "select_top_terms")
    graph.add_edge("select_top_terms", "translate_node")
    graph.add_edge("translate_node", "finalize_translations")
    graph.add_edge("finalize_translations", END)
    return graph.compile()


# ===================== ğŸ”Ÿ æ‰‹åŠ¨æµ‹è¯• =====================
if __name__ == "__main__":
    text = "This study proposes a multimodal deep learning framework for semantic segmentation of remote sensing imagery, aiming to address the well-known trade-off between the spatial resolution of panchromatic images and the spectral richness of hyperspectral data. We construct a dual-branch encoder in which one branch focuses on hyperspectral feature extraction and the other on panchromatic spatial enhancement."
    topic = "Multimodal remote sensing image semantic segmentation â€” emphasizing cross-modal feature fusion, adaptive attention mechanisms, hybrid loss design, and multi-scale supervision to improve segmentation accuracy and generalization across domains"

    state = {
        "text": text,
        "candidates": [],
        "terms": [],
        "topic": topic,
        "translations": {},
        "final_translations": {},
    }

    workflow = build_graph()
    from typing import Any
    result: Any = workflow.invoke(state)  # type: ignore

    print("âœ… å€™é€‰è¯æ•°é‡:", len(result["candidates"]))
    print("âœ… ç¿»è¯‘å€™é€‰:", result.get("translations"))
    print("âœ… æœ€ç»ˆç¿»è¯‘:", result.get("final_translations"))
    # debug counters
    try:
        print("LLM_CALLS:", LLM_CALLS)
        print("DICT_CALLS:", DICT_CALLS)
    except Exception:
        pass


# ===================== æ–°å¢ï¼šä»…æ‰§è¡Œæœ¯è¯­æå–ä¸ç­›é€‰ï¼Œä¸åšç¿»è¯‘çš„å·¥ä½œæµ =====================
@timed_node()
def build_graph_terms_only():
    """ä»…æ‰§è¡Œæœ¯è¯­æå–ä¸ç­›é€‰ï¼Œä¸åšç¿»è¯‘ï¼Œä¾¿äºåç½®æ‰¹é‡ç¿»è¯‘ã€‚"""
    graph = StateGraph(dict)  # type: ignore
    graph.add_node("extract_candidates", extract_candidates)  # type: ignore
    graph.add_node("filter_terms", filter_terms)  # type: ignore
    graph.add_node("select_top_terms", select_top_terms)  # type: ignore
    graph.set_entry_point("extract_candidates")
    graph.add_edge("extract_candidates", "filter_terms")
    graph.add_edge("filter_terms", "select_top_terms")
    graph.add_edge("select_top_terms", END)
    return graph.compile()

